# 231013
# 관통 PJT


## 파이썬으로 웹페이지에 있는 정보를 가져오는 방법
1. 누군가 업로드 해 둔 데이터 다운받기
2. 누군가 만들어 둔 API 서버를 활용하여 정보 받아오기
3. 사람이 검색하는 것처럼 파이썬이 자동으로 검색 후 결과를 수집하는 방법
  - 이러한 기술을 크롤링 이라고 한다

## 데이터 사이언스 프로세스
1. 문제 정의 : 해결하고자 하는 문제 정의
2. 데이터 수집 : 문제 해결에 필요한 데이터 수집
3. 데이터 전처리(정제) : 실질적인 분석을 수행하기 위해 데이터를 가공하는 단계
   1. 수집한 데이터의 오류 제거(결측치, 이상치), 데이터 형식 변환 등
   2. 이 때, pandas 나 numpy 등 사용했었음...
4. 데이터 분석 : 전처리가 완료된 데이터에서 필요한 정보를 추출하는 단계
5. 결과 해석 및 공유 : 의사 결정에 활용하기 위해 결과를 해석하고 시각화

## 데이터 수집
1. 웹 스크래핑 : 웹 페이지에서 데이터를 추출하는 기술
2. 웹 크롤링 : 웹 페이지를 자동으로 탐색하고 데이터를 수집하는 기술
- Open API 활용 : 공개된 API를 통해 데이터 수집
- 데이터 공유 플랫폼 활용 : 캐글, Data.world, 데이콘, 공공데이터포털 등

## 웹 크롤링?
- 여러 페이지를 돌아다니며 원하는 정보 get!
- 웹 사이트들을 돌아다니며 **필요한 데이터를 추출하여 활용할 수 있도록 자동화된 프로세스**

## 웹 크롤링 프로세스
1. 웹 페이지 다운로드 : 해당 웹페이지의 HTML, CSS, JavaScript 등의 코드를 가져옴
2. 페이지 파싱 : 다운로드 받은 코드를 분석하고 필요한 데이터를 추출
3. 링크 추출 및 다른 페이지 탐색 : 다른 링크를 추출, 다음 단계로 이동하여 원하는 데이터를 추출
4. 데이터 추출 및 저장 : 분석 및 시각화에 사용하기 위해 데이터를 처리하고 저장



# 오후 특별 보강 수업
# GIT 에 관하여...

상태
test.txt 를 생성했다
무슨상태일까? -> untracked

untracked
tracked
  modified
  staged

clone / push / pull


git branch first -> first 브랜치 작성
git branch -> 브랜치 목록
git switch first -> first 브랜치로 전환

